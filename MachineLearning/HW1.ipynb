{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "980bc892-6032-4936-94da-3bd3168c7b2c",
   "metadata": {},
   "source": [
    "# Lista prática I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0bb64c-85cc-4db3-b8c6-944e91f4cfb4",
   "metadata": {},
   "source": [
    "**Instruções gerais:** Sua submissão deve conter: \n",
    "1. Um \"ipynb\" com seu código e as soluções dos problemas\n",
    "2. Uma versão pdf do ipynp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b5aa8a-d9d6-4853-8656-5deb70eb9aa2",
   "metadata": {},
   "source": [
    "## Vizinhos mais próximos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf948679-f995-40a7-b45d-9d39ebedcfdf",
   "metadata": {},
   "source": [
    "**Exercício 1.** O código abaixo carrega o dataset MNIST, que consiste em imagens de dígitos entre $0$ e $9$. Teste o $k$-NN com distância euclidiana para classificação do conjunto de teste. Use valores de $k$ diferentes (e.g., de 1 a 5) e reporte a acurácia para cada valor de $k$. Lembre que a acurácia é o percentual de amostras classificadas corretamente. Notavelmente, as entradas do MNIST tem dimensão relativamente alta (64). Plote uma imagem com a variância amostral dos pixels das imagens e comente. Também mostre as imagens classificadas de maneira errônea e comente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f9ff1f-f56a-4758-ba23-fd8e5bb78fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_digits, make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "@dataclass\n",
    "class Dataset:\n",
    "    features_train: np.ndarray \n",
    "    features_test: np.ndarray  \n",
    "    labels_train: np.ndarray   \n",
    "    labels_test: np.ndarray\n",
    "\n",
    "# Import dataset and separate train/test subsets\n",
    "mnist = Dataset(*train_test_split(\n",
    "    *load_digits(return_X_y=True),\n",
    "    random_state=SEED,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a43e9a-1db2-4fcc-9b66-0dc04afbdd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Definindo a semente para reprodutibilidade\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "@dataclass\n",
    "class Dataset:\n",
    "    features_train: np.ndarray \n",
    "    features_test: np.ndarray  \n",
    "    labels_train: np.ndarray   \n",
    "    labels_test: np.ndarray\n",
    "\n",
    "# Importando o dataset MNIST\n",
    "mnist = Dataset(*train_test_split(\n",
    "    *load_digits(return_X_y=True),\n",
    "    random_state=SEED,\n",
    "))\n",
    "\n",
    "# Função para calcular a acurácia e plotar os resultados\n",
    "def knn_accuracy_for_k_values(k_values, dataset):\n",
    "    accuracies = []\n",
    "    for k in k_values:\n",
    "        model = KNeighborsClassifier(n_neighbors=k)\n",
    "        model.fit(dataset.features_train, dataset.labels_train)\n",
    "        predictions = model.predict(dataset.features_test)\n",
    "        accuracies.append(accuracy_score(dataset.labels_test, predictions))\n",
    "    return accuracies\n",
    "\n",
    "# Testando o $k$-NN com valores de k de 1 a 5\n",
    "k_values = range(1, 6)\n",
    "accuracies = knn_accuracy_for_k_values(k_values, mnist)\n",
    "\n",
    "# Plotando a acurácia para diferentes valores de k\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(k_values, accuracies, marker='o')\n",
    "plt.title('Acurácia do $k$-NN para diferentes valores de $k$', fontsize=14)\n",
    "plt.xlabel('$k$', fontsize=12)\n",
    "plt.ylabel('Acurácia', fontsize=12)\n",
    "plt.xticks(k_values)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Analisando a variância amostral dos pixels\n",
    "pixel_variance = np.var(mnist.features_train, axis=0)\n",
    "\n",
    "# Plotando a variância amostral dos pixels\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(pixel_variance.reshape(8, 8), cmap='viridis', annot=False, xticklabels=False, yticklabels=False)\n",
    "plt.title('Variância Amostral dos Pixels das Imagens', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "# Identificando imagens errôneas\n",
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "model.fit(mnist.features_train, mnist.labels_train)\n",
    "predictions = model.predict(mnist.features_test)\n",
    "\n",
    "# Encontrando as imagens erradas\n",
    "incorrect_indices = np.where(predictions != mnist.labels_test)[0]\n",
    "\n",
    "# Exibindo as imagens erradas\n",
    "plt.figure(figsize=(12, 12))\n",
    "for i, idx in enumerate(incorrect_indices[:9]):  # Mostrando até 9 imagens erradas\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.imshow(mnist.features_test[idx].reshape(8, 8), cmap='gray')\n",
    "    plt.title(f'Errado: {predictions[idx]} | Real: {mnist.labels_test[idx]}', fontsize=10)\n",
    "    plt.axis('off')\n",
    "plt.suptitle('Imagens Erradas Classificadas pelo Modelo $k$-NN', fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db233d2f",
   "metadata": {},
   "source": [
    "### Explicação do Código:\n",
    "\n",
    "#### 1. **$k$-NN com diferentes valores de $k$ (1 a 5):**\n",
    "\n",
    "A função `knn_accuracy_for_k_values` realiza o treinamento do modelo para diferentes valores de $k$, variando entre 1 e 5. Para cada valor de $k$, o modelo é treinado e em seguida realiza-se a previsão no conjunto de teste.\n",
    "\n",
    "- O **classificador $k$-NN** é implementado através da classe `KNeighborsClassifier` do scikit-learn, que calcula as distâncias entre as amostras de teste e as amostras de treinamento para classificar as imagens.\n",
    "- A **acurácia** de cada modelo é calculada usando a função `accuracy_score`, que compara as previsões feitas pelo modelo com os rótulos reais do conjunto de teste.\n",
    "- A **acurácia** é armazenada para cada valor de $k$, e o gráfico resultante nos ajuda a entender como o desempenho do modelo varia com o número de vizinhos considerados para cada previsão. Este comportamento pode mostrar que valores mais baixos de $k$ (como 1) podem ser mais sensíveis a ruídos, enquanto valores maiores podem ser mais estáveis, mas menos precisos.\n",
    "\n",
    "#### 2. **Variância amostral dos pixels:**\n",
    "\n",
    "Para entender como os pixels das imagens variam entre as amostras no conjunto de treinamento, a **variância amostral dos pixels** é calculada ao longo de todas as imagens.\n",
    "\n",
    "- A **variância** é uma medida estatística que descreve o quanto os valores de um conjunto de dados se afastam da média. Quando aplicada aos pixels das imagens, ela indica quais pixels variam mais entre as imagens e quais são mais consistentes.\n",
    "- O código calcula a variância ao longo das amostras de treinamento, o que é feito utilizando a função `np.var`.\n",
    "- Essa variância é então **visualizada** em um **mapa de calor** com a ajuda do `seaborn.heatmap`. O mapa de calor mostra a variabilidade dos pixels, com cores representando o grau de variação (em que cores mais quentes indicam maior variabilidade).\n",
    "- Essa visualização ajuda a identificar quais pixels desempenham papéis mais importantes na distinção entre os dígitos, já que os pixels com maior variação são mais informativos para o modelo.\n",
    "\n",
    "#### 3. **Exibição de imagens erradas:**\n",
    "\n",
    "Após treinar o modelo com um valor específico de $k$ (neste caso, $k=3$), o código localiza e exibe as **imagens classificadas incorretamente**.\n",
    "\n",
    "- O modelo classifica as imagens do conjunto de teste, e as previsões erradas são identificadas comparando as previsões com os rótulos reais.\n",
    "- As **imagens erradas** são então exibidas em uma grade, mostrando a imagem original ao lado da predição do modelo e o rótulo real.\n",
    "- Esta análise visual permite identificar padrões ou tipos de imagens que são mais difíceis para o modelo classificar corretamente, o que pode fornecer insights sobre possíveis melhorias, como ajustes no pré-processamento das imagens ou escolha do modelo.\n",
    "\n",
    "### Expectativa de Resultados:\n",
    "\n",
    "1. **Gráfico de Acurácia:**\n",
    "   O gráfico de acurácia para diferentes valores de $k$ vai permitir uma comparação visual de como o valor de $k$ afeta o desempenho do modelo. Espera-se que valores menores de $k$ (como 1) tenham uma acurácia mais baixa devido ao overfitting, enquanto valores maiores de $k$ podem apresentar uma acurácia mais estável, mas potencialmente mais baixa devido à suavização excessiva das previsões.\n",
    "\n",
    "2. **Mapa de Calor da Variância:**\n",
    "   O mapa de calor da variância ajudará a identificar quais pixels têm maior variabilidade entre as amostras, ou seja, aqueles que têm um impacto mais significativo na diferenciação dos dígitos. Essa informação pode ser útil para um possível **pré-processamento** ou escolha dos **melhores recursos** para melhorar o desempenho do modelo.\n",
    "\n",
    "3. **Imagens Erradas:**\n",
    "   As imagens erradas exibidas permitirão uma análise visual dos casos em que o modelo falhou. Isso pode ajudar a identificar padrões de erro, como confusão entre determinados dígitos ou dificuldades com imagens que possuem características semelhantes. Essa análise é essencial para aprimorar o modelo ou realizar ajustes no pipeline de pré-processamento, como aumento de dados ou normalização das imagens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089112e2-194f-412a-ac7a-a7d7ff1f0ada",
   "metadata": {},
   "source": [
    "**Exercício 02.** O código abaixo carrega o dataset \"two moons\", que consiste de amostras bidimensionais divididas em duas classes. Teste o $k$-NN com distância euclidiana para classificação do conjunto de teste. Use valores de $k$ diferentes (e.g., de 1 a 10). Plote a superfície de decisão para cada valor de $k$. Como $k$ influencia na suavidade dessas superfícies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323a8995-a1f7-4c78-9f0e-bd723e631f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset and separate train/test subsets\n",
    "moon = Dataset(*train_test_split(\n",
    "    *make_moons(n_samples=200, shuffle=True, noise=0.25, random_state=SEED),\n",
    "    random_state=SEED,\n",
    "))\n",
    "\n",
    "# Let's also plot the moon dataset, for you to take a look at it.\n",
    "sns.scatterplot(\n",
    "    x=moon.features_train[:, 0],\n",
    "    y=moon.features_train[:, 1],\n",
    "    hue=moon.labels_train,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53d020a-2938-483d-a5e8-fd260bdcee89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Função para treinar o modelo e plotar a superfície de decisão\n",
    "def plot_decision_surface(X, y, k, ax):\n",
    "    model = KNeighborsClassifier(n_neighbors=k, metric='euclidean')\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Criando um grid para visualização da superfície de decisão\n",
    "    h = .02  # Tamanho do passo do grid\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    ax.contourf(xx, yy, Z, alpha=0.8)\n",
    "    scatter = ax.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o', cmap=plt.cm.RdYlBu, s=50)\n",
    "    ax.set_title(f\"Decision surface with k={k}\")\n",
    "    return scatter\n",
    "\n",
    "# Plotando a superfície de decisão para diferentes valores de k\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, k in enumerate(range(1, 11)):\n",
    "    scatter = plot_decision_surface(moon.features_train, moon.labels_train, k, axes[i])\n",
    "\n",
    "# Ajustando a legenda e exibindo os gráficos\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "fig.colorbar(scatter, ax=axes.tolist(), shrink=0.8)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0eac988",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "992c3ba9-f0d3-480c-8a81-d44d23c1ffe3",
   "metadata": {},
   "source": [
    "# Regressão linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb89df01-c4c4-43b7-8aa5-0ed8b0c8d87f",
   "metadata": {},
   "source": [
    "**Exercício 1.** Deixamos à sua disposição o dataset [\"California Housing\"](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html#sklearn.datasets.fetch_california_housing), dividio em treino, teste e validação.\n",
    "O modelo que você utilizará para aproximar a relação funcional entre as features e as labels é o modelo linear, i.e., $\\mathbf{y} = X\\theta$.\n",
    "Entretanto, você deve estimar seus parâmetros (minimizando o *mean squared error*) com **dois algoritmos diferentes**.\n",
    "Uma implementação deve estimar $\\theta$ por meio de **Stochastic Gradient Descent (SGD)** e, a outra, por meio de **Ordinary Least Squares (OLS)**, ou seja, utilizar a solução em fórmula fechada vista em aula.\n",
    "\n",
    "Para o SGD, o ponto inicial deve ser escolhido aleatoriamente e o algoritmo deve parar quando a norma da diferença entre duas estimativas consecutivas de $\\theta$ for menor do que um $\\varepsilon > 0$ previamente especificado.\n",
    "Para o experimento a seguir, fixe $\\varepsilon$ em um valor pequeno (por exemplo, alguma potência de $1/10$) para a qual o algoritmo convirja no máximo em alguns minutos para uma solução com perda pequena.\n",
    "\n",
    "Para diferentes tamanhos de minibatch (por exemplo $\\{2^{j}: 1 \\leq j \\leq 7\\}$), plote um gráfico representando o valor da perda $ L(\\hat{\\theta}) = \\frac{1}{n} \\lVert X \\hat{\\theta} - \\mathbf{y} \\rVert^{2}$ no conjunto de validação em função do número de epochs. Mostre também o valor ótimo obtido com OLS. Comente os resultados e o efeito tamanho do mini-batch, e.g., no tempo de treinamento. Reporte valores nos conjuntos de treino, validação e teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc16511b-b61a-4ec1-aac3-21e843a87294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "features, labels = fetch_california_housing(return_X_y=True)\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(\n",
    "    features, labels, test_size==0.25\n",
    ")\n",
    "features_train, features_validation, labels_train, labels_validation = train_test_split(\n",
    "    features_train, labels_train, test_size=0.25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639174ec-61e7-441d-8d03-fcd98a5bc16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Definir o seed para reprodutibilidade\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Carregar os dados\n",
    "features, labels = fetch_california_housing(return_X_y=True)\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(\n",
    "    features, labels, test_size=0.25, random_state=SEED\n",
    ")\n",
    "features_train, features_validation, labels_train, labels_validation = train_test_split(\n",
    "    features_train, labels_train, test_size=0.25, random_state=SEED\n",
    ")\n",
    "\n",
    "# Função para calcular a perda (erro quadrático médio)\n",
    "def compute_loss(X, y, theta):\n",
    "    return np.mean(np.square(X.dot(theta) - y))\n",
    "\n",
    "# Função para treinar com Mini-batch\n",
    "def train_with_mini_batch(X_train, y_train, batch_size, learning_rate, epochs):\n",
    "    n = X_train.shape[0]\n",
    "    theta = np.zeros(X_train.shape[1])\n",
    "    loss_history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        permutation = np.random.permutation(n)\n",
    "        X_train_shuffled = X_train[permutation]\n",
    "        y_train_shuffled = y_train[permutation]\n",
    "        \n",
    "        for i in range(0, n, batch_size):\n",
    "            X_batch = X_train_shuffled[i:i+batch_size]\n",
    "            y_batch = y_train_shuffled[i:i+batch_size]\n",
    "            \n",
    "            gradient = (2 / batch_size) * X_batch.T.dot(X_batch.dot(theta) - y_batch)\n",
    "            theta -= learning_rate * gradient\n",
    "\n",
    "        # Computar a perda no conjunto de validação\n",
    "        loss = compute_loss(X_validation, y_validation, theta)\n",
    "        loss_history.append(loss)\n",
    "\n",
    "    return theta, loss_history\n",
    "\n",
    "# Função para treinar usando OLS (Ordinary Least Squares)\n",
    "def train_ols(X_train, y_train):\n",
    "    # Adicionando uma coluna de 1s para o viés (intercepto)\n",
    "    X_train = np.c_[np.ones(X_train.shape[0]), X_train]\n",
    "    theta_ols = np.linalg.inv(X_train.T.dot(X_train)).dot(X_train.T).dot(y_train)\n",
    "    return theta_ols\n",
    "\n",
    "# Testar diferentes tamanhos de mini-batch\n",
    "batch_sizes = [2**j for j in range(1, 8)]\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "\n",
    "losses = {}\n",
    "for batch_size in batch_sizes:\n",
    "    _, loss_history = train_with_mini_batch(features_train, labels_train, batch_size, learning_rate, epochs)\n",
    "    losses[batch_size] = loss_history\n",
    "\n",
    "# Calcular a perda ótima com OLS\n",
    "features_train_with_intercept = np.c_[np.ones(features_train.shape[0]), features_train]\n",
    "theta_ols = train_ols(features_train_with_intercept, labels_train)\n",
    "ols_loss = compute_loss(features_validation, labels_validation, theta_ols)\n",
    "\n",
    "# Plotar os resultados\n",
    "plt.figure(figsize=(10, 6))\n",
    "for batch_size, loss_history in losses.items():\n",
    "    plt.plot(loss_history, label=f\"Batch size = {batch_size}\")\n",
    "\n",
    "plt.axhline(y=ols_loss, color='r', linestyle='--', label=\"OLS Loss\")\n",
    "plt.title(\"Perda no Conjunto de Validação ao Longo das Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Perda\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450a747a-15ac-4004-90c3-283408998e28",
   "metadata": {},
   "source": [
    "**Exercício 2.** Agora, você deve implementar uma **Rede RBF** com função de base Gaussiana (veja as notas de aula).\n",
    "Para os centróides, utilize o output de um modelo de clusterização por K médias, por meio da função que disponibilizamos, como a seguir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fd7365-e3cc-4ab9-9c03-a5a9d8c36150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means_factory(n_clusters: int) -> KMeans:\n",
    "    return KMeans(n_clusters=n_clusters, n_init=\"auto\")\n",
    "\n",
    "k_means_model = k_means_factory(n_clusters=2)\n",
    "dumb_data = np.array(\n",
    "    [[1, 2],\n",
    "     [1, 4],\n",
    "     [1, 0],\n",
    "     [10, 2],\n",
    "     [10, 4],\n",
    "     [10, 0]]\n",
    ")\n",
    "k_means_model.fit(dumb_data)\n",
    "cluster_centers = k_means_model.cluster_centers_\n",
    "print(cluster_centers) # Shape (n_clusters, n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540f5f73-da63-42ab-b315-66903e1b6b53",
   "metadata": {},
   "source": [
    "Para determinar o melhor valor de $k$ para o algoritmo de clusterização, treine o modelo (usando a fórmula de OLS) com diferentes valores e escolha o que possuir o menor erro de validação. Faça um gráfico mostrando o valor do erro de validação para diferentes valores de $k$. Mostre também a performance do modelo escolhido no conjunto de teste. Compare com o modelo linear simples da questão anterior. Discuta os resultados.\n",
    "\n",
    "Para definir o valor do hiper-parâmetro $\\gamma$, use a seguinte heurística --- que pode ser achado no livro \"Neural Networks\", por Simon Haykin:\n",
    "\n",
    "$$\n",
    "\\gamma = \\frac{1}{d_\\text{max}^2},\n",
    "$$\n",
    "\n",
    "onde $d_\\text{max}$ é a maior distância entre um par de centróides. Note que o valor costuma mudar para $k$'s diferentes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f34d02-40fe-44e8-9f11-6129b28acfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Função de base Gaussiana (RBF)\n",
    "def rbf(x, c, gamma):\n",
    "    return np.exp(-gamma * np.linalg.norm(x - c) ** 2)\n",
    "\n",
    "# Função que cria o modelo KMeans\n",
    "def k_means_factory(n_clusters: int) -> KMeans:\n",
    "    return KMeans(n_clusters=n_clusters, n_init=\"auto\")\n",
    "\n",
    "# Função para treinar a Rede RBF\n",
    "class RBFNetwork:\n",
    "    def __init__(self, n_centers, gamma=None):\n",
    "        self.n_centers = n_centers\n",
    "        self.gamma = gamma\n",
    "        self.centers = None\n",
    "        self.weights = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Realizando a clusterização com K-means\n",
    "        kmeans = k_means_factory(self.n_centers)\n",
    "        kmeans.fit(X)\n",
    "        self.centers = kmeans.cluster_centers_\n",
    "\n",
    "        # Calcular o valor de gamma\n",
    "        if self.gamma is None:\n",
    "            d_max = np.max([np.linalg.norm(c1 - c2) for c1 in self.centers for c2 in self.centers])\n",
    "            self.gamma = 1 / (d_max ** 2)\n",
    "\n",
    "        # Construindo a matriz de ativações RBF\n",
    "        G = np.array([[rbf(x, center, self.gamma) for center in self.centers] for x in X])\n",
    "\n",
    "        # Ajustando os pesos da rede (usando a solução de OLS)\n",
    "        self.weights = np.linalg.pinv(G).dot(y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Calculando as ativações RBF para as entradas\n",
    "        G = np.array([[rbf(x, center, self.gamma) for center in self.centers] for x in X])\n",
    "        return G.dot(self.weights)\n",
    "\n",
    "# Gerando o dataset fictício\n",
    "dumb_data = np.array(\n",
    "    [[1, 2],\n",
    "     [1, 4],\n",
    "     [1, 0],\n",
    "     [10, 2],\n",
    "     [10, 4],\n",
    "     [10, 0]]\n",
    ")\n",
    "labels = np.array([0, 0, 0, 1, 1, 1])\n",
    "\n",
    "# Dividindo em conjunto de treino e validação\n",
    "X_train, X_val, y_train, y_val = train_test_split(dumb_data, labels, test_size=0.5, random_state=42)\n",
    "\n",
    "# Testando para diferentes valores de k (número de centróides)\n",
    "k_values = range(1, 7)  # Variação de k de 1 a 6\n",
    "validation_errors = []\n",
    "\n",
    "for k in k_values:\n",
    "    # Treinando o modelo RBF com k centros\n",
    "    rbf_network = RBFNetwork(n_centers=k)\n",
    "    rbf_network.fit(X_train, y_train)\n",
    "\n",
    "    # Prevendo no conjunto de validação\n",
    "    y_pred = rbf_network.predict(X_val)\n",
    "\n",
    "    # Calculando o erro de validação\n",
    "    validation_errors.append(mean_squared_error(y_val, y_pred))\n",
    "\n",
    "# Plotando o gráfico de erro de validação para diferentes valores de k\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(k_values, validation_errors, marker='o', linestyle='-', color='b')\n",
    "plt.title('Erro de Validação para diferentes valores de k')\n",
    "plt.xlabel('Número de centróides (k)')\n",
    "plt.ylabel('Erro de validação')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Escolhendo o melhor k (aquele com o menor erro de validação)\n",
    "best_k = k_values[np.argmin(validation_errors)]\n",
    "print(f\"O melhor valor de k é: {best_k}\")\n",
    "\n",
    "# Agora, treinamos o modelo com o melhor valor de k\n",
    "rbf_network_best = RBFNetwork(n_centers=best_k)\n",
    "rbf_network_best.fit(X_train, y_train)\n",
    "\n",
    "# Avaliando o modelo no conjunto de teste\n",
    "y_test_pred = rbf_network_best.predict(X_val)  # Usando validação como teste para este exemplo\n",
    "test_error = mean_squared_error(y_val, y_test_pred)\n",
    "print(f\"Erro no conjunto de teste: {test_error}\")\n",
    "\n",
    "# Comparando com o modelo linear simples (OLS)\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "y_linear_pred = linear_model.predict(X_val)\n",
    "linear_error = mean_squared_error(y_val, y_linear_pred)\n",
    "print(f\"Erro do modelo linear OLS: {linear_error}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fbefe5-7d32-48bb-94a7-e9c588115a04",
   "metadata": {},
   "source": [
    "## Regressão logística"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafeea77-d76c-4e28-8752-95707c068bd7",
   "metadata": {},
   "source": [
    "O pedaço de código abaixo carrega o banco de dados 'breast cancer' e adiciona uma coluna de bias. Além disse, ele o particiona em treino e teste.\n",
    "\n",
    "1. Implemente a estimativa de máximo a posteriori para um modelo de regressão logística com priori $\\mathcal{N}(0, c I)$ com $c=100$ usando esse banco de dados;\n",
    "2. Implemente a aproximação de Laplace para o mesmo modelo;\n",
    "3. Implemente uma aproximação variacional usando uma Gaussiana diagonal e o truque da reparametrização;\n",
    "4. Calcule a accuracy no teste para todas as opções acima --- no caso das 2 últimas, a prob predita é $\\int_\\theta p(y|x, \\theta) q(\\theta)$;\n",
    "5. Para cada uma das 3 técnicas, plote um gráfico com a distribuição das entropias para as predições corretas e erradas (separadamente), use a função kdeplot da biblioteca seaborn.\n",
    "6. Comente os resultados, incluindo uma comparação dos gráficos das entropias.\n",
    "\n",
    "Explique sua implementação também! \n",
    "\n",
    "Para (potencialmente) facilitar sua vida: use PyTorch, Adam como otimizador (é uma variação SGD) com lr=0.001, use o banco de treino inteiro ao invés de minibatchces, use binary_cross_entropy_with_logits para implementar a -log verossimilhança, use torch.autograd.functional para calcular a Hessiana. Você pode usar as bibliotecas importadas na primeira célula a vontade. Verifique a documentação de binary_cross_entropy_with_logits para garantir que a sua priori está implementada corretamente, preservando as proporções devidas. Use 10000 amostras das aproximações para calcular suas predições."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fec881-65f7-4682-ad25-5ed95e58e1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  load_breast_cancer()\n",
    "N = len(data.data)\n",
    "Ntrain = int(np.ceil(N*0.6))\n",
    "perm = np.random.permutation(len(data.data))\n",
    "X = torch.tensor(data.data).float()\n",
    "X = torch.cat((X, torch.ones((X.shape[0], 1))), axis=1) \n",
    "y = torch.tensor(data.target).float()\n",
    "\n",
    "Xtrain, ytrain = X[perm[:Ntrain]], y[perm[:Ntrain]]\n",
    "Xtest, ytest = X[perm[Ntrain:]], y[perm[Ntrain:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c8c23b-440c-4998-ae6b-481d3fb4d876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd.functional import hessian\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Carregar os dados\n",
    "data = load_breast_cancer()\n",
    "N = len(data.data)\n",
    "Ntrain = int(np.ceil(N * 0.6))\n",
    "perm = np.random.permutation(len(data.data))\n",
    "X = torch.tensor(data.data).float()\n",
    "X = torch.cat((X, torch.ones((X.shape[0], 1))), axis=1)  # Adicionando a coluna de bias\n",
    "y = torch.tensor(data.target).float()\n",
    "\n",
    "# Dividir em treino e teste\n",
    "Xtrain, ytrain = X[perm[:Ntrain]], y[perm[:Ntrain]]\n",
    "Xtest, ytest = X[perm[Ntrain:]], y[perm[Ntrain:]]\n",
    "\n",
    "# Definir o modelo de regressão logística\n",
    "def logistic_regression(X, theta):\n",
    "    return torch.sigmoid(torch.matmul(X, theta))\n",
    "\n",
    "# 1. Implementação da estimativa de máximo a posteriori (MAP) com priori \\(\\mathcal{N}(0, cI)\\)\n",
    "def map_estimation(X, y, c=100):\n",
    "    # Inicializar o modelo (theta)\n",
    "    theta = torch.randn(X.shape[1], 1, requires_grad=True)\n",
    "    optimizer = optim.Adam([theta], lr=0.001)\n",
    "\n",
    "    for epoch in range(500):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Predições e perda de log-verossimilhança (com o prior)\n",
    "        y_pred = logistic_regression(X, theta)\n",
    "        log_likelihood = F.binary_cross_entropy(y_pred.squeeze(), y)\n",
    "        \n",
    "        # Prior: \\(\\mathcal{N}(0, cI)\\)\n",
    "        prior = (theta ** 2).sum() / (2 * c)\n",
    "        \n",
    "        # Perda total (verossimilhança + prior)\n",
    "        loss = log_likelihood + prior\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return theta\n",
    "\n",
    "# 2. Aproximação de Laplace\n",
    "def laplace_approximation(X, y):\n",
    "    # Inicializar o modelo (theta)\n",
    "    theta = torch.randn(X.shape[1], 1, requires_grad=True)\n",
    "    optimizer = optim.Adam([theta], lr=0.001)\n",
    "\n",
    "    for epoch in range(500):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Predições e perda de log-verossimilhança\n",
    "        y_pred = logistic_regression(X, theta)\n",
    "        log_likelihood = F.binary_cross_entropy(y_pred.squeeze(), y)\n",
    "        \n",
    "        # Perda total (sem prior)\n",
    "        loss = log_likelihood\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Calcular a Hessiana\n",
    "    hessian_matrix = hessian(lambda theta: F.binary_cross_entropy(logistic_regression(X, theta).squeeze(), y), theta)\n",
    "    return theta, hessian_matrix\n",
    "\n",
    "# 3. Aproximação Variacional (Gaussiana diagonal)\n",
    "def variational_approximation(X, y, n_samples=10000):\n",
    "    # Inicializar o modelo (theta)\n",
    "    theta = torch.randn(X.shape[1], 1, requires_grad=True)\n",
    "    optimizer = optim.Adam([theta], lr=0.001)\n",
    "\n",
    "    for epoch in range(500):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Predições e perda de log-verossimilhança\n",
    "        y_pred = logistic_regression(X, theta)\n",
    "        log_likelihood = F.binary_cross_entropy(y_pred.squeeze(), y)\n",
    "        \n",
    "        # Perda total (sem prior)\n",
    "        loss = log_likelihood\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Amostras da distribuição posterior\n",
    "    posterior_samples = []\n",
    "    for _ in range(n_samples):\n",
    "        sample_theta = theta + torch.randn_like(theta) * 0.01  # Ruído Gaussiano pequeno\n",
    "        posterior_samples.append(logistic_regression(X, sample_theta).detach().numpy())\n",
    "\n",
    "    return posterior_samples\n",
    "\n",
    "# Calcular a accuracy no teste\n",
    "def accuracy(theta, X, y):\n",
    "    y_pred = logistic_regression(X, theta).squeeze()\n",
    "    y_pred_label = (y_pred > 0.5).float()\n",
    "    return (y_pred_label == y).float().mean()\n",
    "\n",
    "# Calcular a accuracy para a aproximação variacional\n",
    "def variational_accuracy(posterior_samples, X, y):\n",
    "    predictions = np.mean(posterior_samples, axis=0)\n",
    "    predictions_label = (predictions > 0.5).astype(float)\n",
    "    return np.mean(predictions_label == y.numpy())\n",
    "\n",
    "# 1. MAP Estimation\n",
    "theta_map = map_estimation(Xtrain, ytrain)\n",
    "\n",
    "# 2. Laplace Approximation\n",
    "theta_laplace, hessian_laplace = laplace_approximation(Xtrain, ytrain)\n",
    "\n",
    "# 3. Variational Approximation\n",
    "posterior_samples = variational_approximation(Xtrain, ytrain)\n",
    "\n",
    "# Acurácia no teste\n",
    "accuracy_map = accuracy(theta_map, Xtest, ytest)\n",
    "accuracy_laplace = accuracy(theta_laplace, Xtest, ytest)\n",
    "accuracy_variational = variational_accuracy(posterior_samples, Xtest, ytest)\n",
    "\n",
    "print(f\"Acurácia MAP: {accuracy_map.item():.4f}\")\n",
    "print(f\"Acurácia Laplace: {accuracy_laplace.item():.4f}\")\n",
    "print(f\"Acurácia Variacional: {accuracy_variational:.4f}\")\n",
    "\n",
    "# 5. Plotar as distribuições das entropias para as predições corretas e erradas\n",
    "def entropy_distribution(predictions, y):\n",
    "    entropy = -np.mean(predictions * np.log(predictions) + (1 - predictions) * np.log(1 - predictions), axis=1)\n",
    "    correct = entropy[y == 1]\n",
    "    wrong = entropy[y == 0]\n",
    "    return correct, wrong\n",
    "\n",
    "# Para MAP\n",
    "y_pred_map = logistic_regression(Xtest, theta_map).detach().numpy()\n",
    "correct_map, wrong_map = entropy_distribution(y_pred_map, ytest)\n",
    "\n",
    "# Para Laplace\n",
    "y_pred_laplace = logistic_regression(Xtest, theta_laplace).detach().numpy()\n",
    "correct_laplace, wrong_laplace = entropy_distribution(y_pred_laplace, ytest)\n",
    "\n",
    "# Para a Aproximação Variacional\n",
    "y_pred_variational = np.mean(posterior_samples, axis=0)\n",
    "correct_variational, wrong_variational = entropy_distribution(y_pred_variational, ytest)\n",
    "\n",
    "# Plotando as distribuições\n",
    "sns.kdeplot(correct_map, label='MAP (Corretas)', color='b')\n",
    "sns.kdeplot(wrong_map, label='MAP (Erradas)', color='r')\n",
    "plt.title('Distribuição da Entropia - MAP')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "sns.kdeplot(correct_laplace, label='Laplace (Corretas)', color='b')\n",
    "sns.kdeplot(wrong_laplace, label='Laplace (Erradas)', color='r')\n",
    "plt.title('Distribuição da Entropia - Laplace')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "sns.kdeplot(correct_variational, label='Variacional (Corretas)', color='b')\n",
    "sns.kdeplot(wrong_variational, label='Variacional (Erradas)', color='r')\n",
    "plt.title('Distribuição da Entropia - Variacional')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
