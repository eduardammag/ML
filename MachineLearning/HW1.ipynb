{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "980bc892-6032-4936-94da-3bd3168c7b2c",
   "metadata": {},
   "source": [
    "# Lista prática I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0bb64c-85cc-4db3-b8c6-944e91f4cfb4",
   "metadata": {},
   "source": [
    "**Instruções gerais:** Sua submissão deve conter: \n",
    "1. Um \"ipynb\" com seu código e as soluções dos problemas\n",
    "2. Uma versão pdf do ipynp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b5aa8a-d9d6-4853-8656-5deb70eb9aa2",
   "metadata": {},
   "source": [
    "## Vizinhos mais próximos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf948679-f995-40a7-b45d-9d39ebedcfdf",
   "metadata": {},
   "source": [
    "**Exercício 1.** O código abaixo carrega o dataset MNIST, que consiste em imagens de dígitos entre $0$ e $9$. Teste o $k$-NN com distância euclidiana para classificação do conjunto de teste. Use valores de $k$ diferentes (e.g., de 1 a 5) e reporte a acurácia para cada valor de $k$. Lembre que a acurácia é o percentual de amostras classificadas corretamente. Notavelmente, as entradas do MNIST tem dimensão relativamente alta (64). Plote uma imagem com a variância amostral dos pixels das imagens e comente. Também mostre as imagens classificadas de maneira errônea e comente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f9ff1f-f56a-4758-ba23-fd8e5bb78fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_digits, make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "@dataclass\n",
    "class Dataset:\n",
    "    features_train: np.ndarray \n",
    "    features_test: np.ndarray  \n",
    "    labels_train: np.ndarray   \n",
    "    labels_test: np.ndarray\n",
    "\n",
    "# Import dataset and separate train/test subsets\n",
    "mnist = Dataset(*train_test_split(\n",
    "    *load_digits(return_X_y=True),\n",
    "    random_state=SEED,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a43e9a-1db2-4fcc-9b66-0dc04afbdd98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "089112e2-194f-412a-ac7a-a7d7ff1f0ada",
   "metadata": {},
   "source": [
    "**Exercício 02.** O código abaixo carrega o dataset \"two moons\", que consiste de amostras bidimensionais divididas em duas classes. Teste o $k$-NN com distância euclidiana para classificação do conjunto de teste. Use valores de $k$ diferentes (e.g., de 1 a 10). Plote a superfície de decisão para cada valor de $k$. Como $k$ influencia na suavidade dessas superfícies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323a8995-a1f7-4c78-9f0e-bd723e631f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset and separate train/test subsets\n",
    "moon = Dataset(*train_test_split(\n",
    "    *make_moons(n_samples=200, shuffle=True, noise=0.25, random_state=SEED),\n",
    "    random_state=SEED,\n",
    "))\n",
    "\n",
    "# Let's also plot the moon dataset, for you to take a look at it.\n",
    "sns.scatterplot(\n",
    "    x=moon.features_train[:, 0],\n",
    "    y=moon.features_train[:, 1],\n",
    "    hue=moon.labels_train,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53d020a-2938-483d-a5e8-fd260bdcee89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "992c3ba9-f0d3-480c-8a81-d44d23c1ffe3",
   "metadata": {},
   "source": [
    "# Regressão linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb89df01-c4c4-43b7-8aa5-0ed8b0c8d87f",
   "metadata": {},
   "source": [
    "**Exercício 1.** Deixamos à sua disposição o dataset [\"California Housing\"](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html#sklearn.datasets.fetch_california_housing), dividio em treino, teste e validação.\n",
    "O modelo que você utilizará para aproximar a relação funcional entre as features e as labels é o modelo linear, i.e., $\\mathbf{y} = X\\theta$.\n",
    "Entretanto, você deve estimar seus parâmetros (minimizando o *mean squared error*) com **dois algoritmos diferentes**.\n",
    "Uma implementação deve estimar $\\theta$ por meio de **Stochastic Gradient Descent (SGD)** e, a outra, por meio de **Ordinary Least Squares (OLS)**, ou seja, utilizar a solução em fórmula fechada vista em aula.\n",
    "\n",
    "Para o SGD, o ponto inicial deve ser escolhido aleatoriamente e o algoritmo deve parar quando a norma da diferença entre duas estimativas consecutivas de $\\theta$ for menor do que um $\\varepsilon > 0$ previamente especificado.\n",
    "Para o experimento a seguir, fixe $\\varepsilon$ em um valor pequeno (por exemplo, alguma potência de $1/10$) para a qual o algoritmo convirja no máximo em alguns minutos para uma solução com perda pequena.\n",
    "\n",
    "Para diferentes tamanhos de minibatch (por exemplo $\\{2^{j}: 1 \\leq j \\leq 7\\}$), plote um gráfico representando o valor da perda $ L(\\hat{\\theta}) = \\frac{1}{n} \\lVert X \\hat{\\theta} - \\mathbf{y} \\rVert^{2}$ no conjunto de validação em função do número de epochs. Mostre também o valor ótimo obtido com OLS. Comente os resultados e o efeito tamanho do mini-batch, e.g., no tempo de treinamento. Reporte valores nos conjuntos de treino, validação e teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc16511b-b61a-4ec1-aac3-21e843a87294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "features, labels = fetch_california_housing(return_X_y=True)\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(\n",
    "    features, labels, test_size==0.25\n",
    ")\n",
    "features_train, features_validation, labels_train, labels_validation = train_test_split(\n",
    "    features_train, labels_train, test_size=0.25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639174ec-61e7-441d-8d03-fcd98a5bc16e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "450a747a-15ac-4004-90c3-283408998e28",
   "metadata": {},
   "source": [
    "**Exercício 2.** Agora, você deve implementar uma **Rede RBF** com função de base Gaussiana (veja as notas de aula).\n",
    "Para os centróides, utilize o output de um modelo de clusterização por K médias, por meio da função que disponibilizamos, como a seguir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fd7365-e3cc-4ab9-9c03-a5a9d8c36150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means_factory(n_clusters: int) -> KMeans:\n",
    "    return KMeans(n_clusters=n_clusters, n_init=\"auto\")\n",
    "\n",
    "k_means_model = k_means_factory(n_clusters=2)\n",
    "dumb_data = np.array(\n",
    "    [[1, 2],\n",
    "     [1, 4],\n",
    "     [1, 0],\n",
    "     [10, 2],\n",
    "     [10, 4],\n",
    "     [10, 0]]\n",
    ")\n",
    "k_means_model.fit(dumb_data)\n",
    "cluster_centers = k_means_model.cluster_centers_\n",
    "print(cluster_centers) # Shape (n_clusters, n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540f5f73-da63-42ab-b315-66903e1b6b53",
   "metadata": {},
   "source": [
    "Para determinar o melhor valor de $k$ para o algoritmo de clusterização, treine o modelo (usando a fórmula de OLS) com diferentes valores e escolha o que possuir o menor erro de validação. Faça um gráfico mostrando o valor do erro de validação para diferentes valores de $k$. Mostre também a performance do modelo escolhido no conjunto de teste. Compare com o modelo linear simples da questão anterior. Discuta os resultados.\n",
    "\n",
    "Para definir o valor do hiper-parâmetro $\\gamma$, use a seguinte heurística --- que pode ser achado no livro \"Neural Networks\", por Simon Haykin:\n",
    "\n",
    "$$\n",
    "\\gamma = \\frac{1}{d_\\text{max}^2},\n",
    "$$\n",
    "\n",
    "onde $d_\\text{max}$ é a maior distância entre um par de centróides. Note que o valor costuma mudar para $k$'s diferentes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f34d02-40fe-44e8-9f11-6129b28acfad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3fbefe5-7d32-48bb-94a7-e9c588115a04",
   "metadata": {},
   "source": [
    "## Regressão logística"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafeea77-d76c-4e28-8752-95707c068bd7",
   "metadata": {},
   "source": [
    "O pedaço de código abaixo carrega o banco de dados 'breast cancer' e adiciona uma coluna de bias. Além disse, ele o particiona em treino e teste.\n",
    "\n",
    "1. Implemente a estimativa de máximo a posteriori para um modelo de regressão logística com priori $\\mathcal{N}(0, c I)$ com $c=100$ usando esse banco de dados;\n",
    "2. Implemente a aproximação de Laplace para o mesmo modelo;\n",
    "3. Implemente uma aproximação variacional usando uma Gaussiana diagonal e o truque da reparametrização;\n",
    "4. Calcule a accuracy no teste para todas as opções acima --- no caso das 2 últimas, a prob predita é $\\int_\\theta p(y|x, \\theta) q(\\theta)$;\n",
    "5. Para cada uma das 3 técnicas, plote um gráfico com a distribuição das entropias para as predições corretas e erradas (separadamente), use a função kdeplot da biblioteca seaborn.\n",
    "6. Comente os resultados, incluindo uma comparação dos gráficos das entropias.\n",
    "\n",
    "Explique sua implementação também! \n",
    "\n",
    "Para (potencialmente) facilitar sua vida: use PyTorch, Adam como otimizador (é uma variação SGD) com lr=0.001, use o banco de treino inteiro ao invés de minibatchces, use binary_cross_entropy_with_logits para implementar a -log verossimilhança, use torch.autograd.functional para calcular a Hessiana. Você pode usar as bibliotecas importadas na primeira célula a vontade. Verifique a documentação de binary_cross_entropy_with_logits para garantir que a sua priori está implementada corretamente, preservando as proporções devidas. Use 10000 amostras das aproximações para calcular suas predições."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fec881-65f7-4682-ad25-5ed95e58e1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  load_breast_cancer()\n",
    "N = len(data.data)\n",
    "Ntrain = int(np.ceil(N*0.6))\n",
    "perm = np.random.permutation(len(data.data))\n",
    "X = torch.tensor(data.data).float()\n",
    "X = torch.cat((X, torch.ones((X.shape[0], 1))), axis=1) \n",
    "y = torch.tensor(data.target).float()\n",
    "\n",
    "Xtrain, ytrain = X[perm[:Ntrain]], y[perm[:Ntrain]]\n",
    "Xtest, ytest = X[perm[Ntrain:]], y[perm[Ntrain:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c8c23b-440c-4998-ae6b-481d3fb4d876",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
